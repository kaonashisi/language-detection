{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85c19e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c523646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_df():\n",
    "    path_train = '../data/new/train/'\n",
    "    path_test = '../data/new/train/'\n",
    "\n",
    "    train_list = os.listdir(path_train)\n",
    "    test_list = os.listdir(path_test)\n",
    "    \n",
    "    d_train = {'language': [x[:2] for x in train_list], 'audio_path': [path_train + x for x in train_list], 'set' : 'train'}\n",
    "    d_test = {'language': [x[:2] for x in test_list], 'audio_path': [path_test + x for x in test_list], 'set' : 'test'}\n",
    "    \n",
    "    df_train = pd.DataFrame(data=d_train)\n",
    "    df_test = pd.DataFrame(data=d_test)\n",
    "    frames = [df_train, df_test]\n",
    "    \n",
    "    return pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9cdb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sql_table(df):\n",
    "    pw = os.getenv('MySQLpwd')\n",
    "    connection_string = 'mysql+pymysql://root:' + pw + '@localhost:3306/'\n",
    "    engine = create_engine(connection_string)\n",
    "    df.to_sql('language_audio_sets', engine, 'languages', if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9192dcf0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/new/train/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_test_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#create_sql_table(df)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m, in \u001b[0;36mcreate_train_test_df\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m path_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/new/train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m path_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/new/train/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m train_list \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test_list \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(path_test)\n\u001b[1;32m      8\u001b[0m d_train \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlanguage\u001b[39m\u001b[38;5;124m'\u001b[39m: [x[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_list], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_path\u001b[39m\u001b[38;5;124m'\u001b[39m: [path_train \u001b[38;5;241m+\u001b[39m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m train_list], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mset\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/new/train/'"
     ]
    }
   ],
   "source": [
    "df = create_train_test_df()\n",
    "#create_sql_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dc612",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98bebc8",
   "metadata": {},
   "source": [
    "## Generate features for samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18134ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "import common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "FB_HEIGHT = 40  # filter banks\n",
    "WIDTH = 1000\n",
    "COLOR_DEPTH = 1\n",
    "INPUT_SHAPE = (FB_HEIGHT, WIDTH, COLOR_DEPTH)\n",
    "\n",
    "DATA_TYPE = 'float32'\n",
    "DATA_KEY = 'data'\n",
    "\n",
    "LANGUAGES = ['en', 'de', 'es']\n",
    "GENDERS = ['m', 'f']\n",
    "\n",
    "LANGUAGE_INDEX = 0\n",
    "GENDER_INDEX = 1\n",
    "\n",
    "THRESHOLD = 0.8\n",
    "\n",
    "FRAGMENT_DURATION = 10  # seconds\n",
    "\n",
    "DATASET_DIST = '../data/spoken_language_audio_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4180cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fb_and_mfcc(signal, sample_rate):\n",
    "\n",
    "    # Pre-Emphasis\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(\n",
    "        signal[0],\n",
    "        signal[1:] - pre_emphasis * signal[:-1])\n",
    "\n",
    "    # Framing\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "\n",
    "    # Convert from seconds to samples\n",
    "    frame_length, frame_step = (\n",
    "        frame_size * sample_rate,\n",
    "        frame_stride * sample_rate)\n",
    "    signal_length = len(emphasized_signal)\n",
    "    frame_length = int(round(frame_length))\n",
    "    frame_step = int(round(frame_step))\n",
    "\n",
    "    # Make sure that we have at least 1 frame\n",
    "    num_frames = int(\n",
    "        np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "\n",
    "    # Pad Signal to make sure that all frames have equal\n",
    "    # number of samples without truncating any samples\n",
    "    # from the original signal\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "\n",
    "    indices = (\n",
    "        np.tile(np.arange(0, frame_length), (num_frames, 1)) +\n",
    "        np.tile(\n",
    "            np.arange(0, num_frames * frame_step, frame_step),\n",
    "            (frame_length, 1)\n",
    "        ).T\n",
    "    )\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "\n",
    "    # Window\n",
    "    frames *= np.hamming(frame_length)\n",
    "\n",
    "    # Fourier-Transform and Power Spectrum\n",
    "    NFFT = 512\n",
    "\n",
    "    # Magnitude of the FFT\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "\n",
    "    # Power Spectrum\n",
    "    pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))\n",
    "\n",
    "    # Filter Banks\n",
    "    nfilt = 40\n",
    "\n",
    "    low_freq_mel = 0\n",
    "\n",
    "    # Convert Hz to Mel\n",
    "    high_freq_mel = (2595 * np.log10(1 + (sample_rate / 2) / 700))\n",
    "\n",
    "    # Equally spaced in Mel scale\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "\n",
    "    # Convert Mel to Hz\n",
    "    hz_points = (700 * (10**(mel_points / 2595) - 1))\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])   # left\n",
    "        f_m = int(bin[m])             # center\n",
    "        f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "\n",
    "    # Numerical Stability\n",
    "    filter_banks = np.where(\n",
    "        filter_banks == 0,\n",
    "        np.finfo(float).eps,\n",
    "        filter_banks)\n",
    "\n",
    "    # dB\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "\n",
    "    return filter_banks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5dcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(input_dir, debug=False):\n",
    "    files = []\n",
    "\n",
    "    extensions = ['*.flac']\n",
    "    for extension in extensions:\n",
    "        files.extend(glob.glob(os.path.join(input_dir, extension)))\n",
    "\n",
    "    for file in files:\n",
    "        print(file)\n",
    "\n",
    "        signal, sample_rate = sf.read(file)\n",
    "        assert len(signal) > 0\n",
    "        assert sample_rate == 22050\n",
    "\n",
    "        fb = generate_fb_and_mfcc(signal, sample_rate)\n",
    "        fb = fb.astype(DATA_TYPE, copy=False)\n",
    "\n",
    "        assert fb.dtype == DATA_TYPE\n",
    "        assert fb.shape == (WIDTH, FB_HEIGHT)\n",
    "\n",
    "        # .npz extension is added automatically\n",
    "        file_without_ext = os.path.splitext(file)[0]\n",
    "\n",
    "        np.savez_compressed(file_without_ext + '.fb', data=fb)\n",
    "\n",
    "        if debug:\n",
    "            end = time.time()\n",
    "            print(\"It took [s]: \", end - start)\n",
    "\n",
    "            # data is casted to uint8, i.e. (0, 255)\n",
    "            import imageio\n",
    "            imageio.imwrite('fb_image.png', fb)\n",
    "\n",
    "            exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2996f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#process_audio(os.path.join(DATASET_DIST, 'test'))\n",
    "#process_audio(os.path.join(DATASET_DIST, 'train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2408cbe7",
   "metadata": {},
   "source": [
    "## Normalize feature and build folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f7d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_uids(uids):\n",
    "    for language in LANGUAGES:\n",
    "        for gender in GENDERS:\n",
    "            if len(uids[language][gender]) == 0:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def generate_fold(\n",
    "        uids,\n",
    "        input_dir,\n",
    "        input_ext,\n",
    "        output_dir,\n",
    "        group,\n",
    "        fold_index,\n",
    "        input_shape,\n",
    "        normalize,\n",
    "        output_shape):\n",
    "\n",
    "    # pull uid for each a language, gender pair\n",
    "    fold_uids = []\n",
    "    for language in LANGUAGES:\n",
    "        for gender in GENDERS:\n",
    "            fold_uids.append(uids[language][gender].pop())\n",
    "\n",
    "    # find files for given uids\n",
    "    fold_files = []\n",
    "    for fold_uid in fold_uids:\n",
    "        filename = '*{uid}*{extension}'.format(\n",
    "            uid=fold_uid,\n",
    "            extension=input_ext)\n",
    "        fold_files.extend(glob(os.path.join(input_dir, filename)))\n",
    "\n",
    "    fold_files = sorted(fold_files)\n",
    "    fold_files = shuffle(fold_files, random_state=SEED)\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    # create a file array\n",
    "    filename = \"{group}_data.fold{index}.npy\".format(\n",
    "        group=group, index=fold_index)\n",
    "    features = np.memmap(\n",
    "        os.path.join(output_dir, filename),\n",
    "        dtype=DATA_TYPE,\n",
    "        mode='w+',\n",
    "        shape=(len(fold_files),) + output_shape)\n",
    "\n",
    "    # append data to a file array\n",
    "    # append metadata to an array\n",
    "    for index, fold_file in enumerate(fold_files):\n",
    "        print(fold_file)\n",
    "\n",
    "        filename = common.get_filename(fold_file)\n",
    "        language = filename.split('_')[0]\n",
    "        gender = filename.split('_')[1]\n",
    "\n",
    "        data = np.load(fold_file)[DATA_KEY]\n",
    "        assert data.shape == input_shape\n",
    "        assert data.dtype == DATA_TYPE\n",
    "\n",
    "        features[index] = normalize(data)\n",
    "        metadata.append((language, gender, filename))\n",
    "\n",
    "    assert len(metadata) == len(fold_files)\n",
    "\n",
    "    # store metadata in a file\n",
    "    filename = \"{group}_metadata.fold{index}.npy\".format(\n",
    "        group=group,\n",
    "        index=fold_index)\n",
    "    np.save(\n",
    "        os.path.join(output_dir, filename),\n",
    "        metadata)\n",
    "\n",
    "    # flush changes to a disk\n",
    "    features.flush()\n",
    "    del features\n",
    "\n",
    "\n",
    "def generate_folds(\n",
    "        input_dir,\n",
    "        input_ext,\n",
    "        output_dir,\n",
    "        group,\n",
    "        input_shape,\n",
    "        normalize,\n",
    "        output_shape):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    files = glob(os.path.join(input_dir, '*' + input_ext))\n",
    "\n",
    "    uids = common.group_uids(files)\n",
    "\n",
    "    fold_index = 1\n",
    "    while has_uids(uids):\n",
    "        print(\"[{group}] Fold {index}\".format(group=group, index=fold_index))\n",
    "\n",
    "        generate_fold(\n",
    "            uids,\n",
    "            input_dir,\n",
    "            input_ext,\n",
    "            output_dir,\n",
    "            group,\n",
    "            fold_index,\n",
    "            input_shape,\n",
    "            normalize,\n",
    "            output_shape)\n",
    "\n",
    "        fold_index += 1\n",
    "\n",
    "\n",
    "def normalize_fb(spectrogram):\n",
    "\n",
    "    # Mean and Variance Normalization\n",
    "    spectrogram = speechpy.processing.cmvn(\n",
    "        spectrogram,\n",
    "        variance_normalization=True)\n",
    "\n",
    "    # MinMax Scaler, scale values between (0,1)\n",
    "    normalized = (\n",
    "        (spectrogram - np.min(spectrogram)) /\n",
    "        (np.max(spectrogram) - np.min(spectrogram))\n",
    "    )\n",
    "\n",
    "    # Rotate 90deg\n",
    "    normalized = np.swapaxes(normalized, 0, 1)\n",
    "\n",
    "    # Reshape, tensor 3d\n",
    "    (height, width) = normalized.shape\n",
    "    normalized = normalized.reshape(height, width, COLOR_DEPTH)\n",
    "\n",
    "    assert normalized.dtype == DATA_TYPE\n",
    "    assert np.max(normalized) == 1.0\n",
    "    assert np.min(normalized) == 0.0\n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6e8daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fb\n",
    "generate_folds(\n",
    "    os.path.join(DATASET_DIST, 'test'),\n",
    "    '.fb.npz',\n",
    "    output_dir='build/folds',\n",
    "    group='test',\n",
    "    input_shape=(WIDTH, FB_HEIGHT),\n",
    "    normalize=normalize_fb,\n",
    "    output_shape=(FB_HEIGHT, WIDTH, COLOR_DEPTH)\n",
    ")\n",
    "generate_folds(\n",
    "    os.path.join(DATASET_DIST, 'train'),\n",
    "    '.fb.npz',\n",
    "    output_dir='build/folds',\n",
    "    group='train',\n",
    "    input_shape=(WIDTH, FB_HEIGHT),\n",
    "    normalize=normalize_fb,\n",
    "    output_shape=(FB_HEIGHT, WIDTH, COLOR_DEPTH)\n",
    ")\n",
    "\n",
    "end = time.time()\n",
    "print(\"It took [s]: \", end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
